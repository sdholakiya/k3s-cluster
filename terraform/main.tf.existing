provider "aws" {
  region = var.aws_region
}

# This configuration deploys K3s to an existing EC2 instance in private subnets.
# REQUIREMENTS:
# 1. The VPC must have SSM VPC endpoints configured for private instances to connect to AWS Systems Manager
# 2. For the SSM connection to work, the VPC needs:
#    - com.amazonaws.[region].ssm
#    - com.amazonaws.[region].ec2messages
#    - com.amazonaws.[region].ssmmessages
# 3. The security group must allow outbound traffic to these endpoints

# Data source to get the existing EC2 instance
data "aws_instance" "existing_instance" {
  instance_id = var.existing_instance_id
}

# Optionally create an IAM role and instance profile for SSM access
resource "aws_iam_role" "ssm_role" {
  count = var.create_iam_role ? 1 : 0
  name  = var.iam_role_name

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "ssm_policy" {
  count      = var.create_iam_role ? 1 : 0
  role       = aws_iam_role.ssm_role[0].name
  policy_arn = "arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore"
}

resource "aws_iam_instance_profile" "ssm_profile" {
  count = var.create_iam_role ? 1 : 0
  name  = "${var.iam_role_name}-profile"
  role  = aws_iam_role.ssm_role[0].name
}

# If create_iam_role is true, attach the role to the existing instance
resource "aws_ec2_instance_state" "update_instance" {
  count         = var.create_iam_role ? 1 : 0
  instance_id   = data.aws_instance.existing_instance.id
  instance_profile_arn = aws_iam_instance_profile.ssm_profile[0].arn
}

# SSM document to install K3s on the existing instance
resource "aws_ssm_document" "install_k3s" {
  name            = "InstallK3s"
  document_type   = "Command"
  document_format = "YAML"
  
  content = <<DOC
schemaVersion: '2.2'
description: 'Install K3s on an existing EC2 instance'
parameters: {}
mainSteps:
  - action: 'aws:runShellScript'
    name: 'installK3s'
    inputs:
      runCommand:
        - '#!/bin/bash'
        - 'apt-get update'
        - 'apt-get install -y curl'
        - '# Install K3s'
        - 'curl -sfL https://get.k3s.io | sh -'
        - '# Wait for K3s to start'
        - 'sleep 30'
        - '# Get K3s token for later use'
        - 'K3S_TOKEN=$(cat /var/lib/rancher/k3s/server/node-token)'
        - '# Output token to file for retrieval'
        - 'echo "K3S_TOKEN=$K3S_TOKEN" > /tmp/k3s-token'
        - '# Ensure k3s is running'
        - 'systemctl status k3s'
        - '# Copy kubeconfig to accessible location'
        - 'cp /etc/rancher/k3s/k3s.yaml /tmp/kubeconfig'
        - 'chmod 644 /tmp/kubeconfig'
DOC
}

# Execute the SSM document on the existing instance
resource "aws_ssm_association" "k3s_install" {
  name = aws_ssm_document.install_k3s.name
  targets {
    key    = "InstanceIds"
    values = [data.aws_instance.existing_instance.id]
  }
  depends_on = [aws_ec2_instance_state.update_instance]
}

# SSM connection to retrieve kubeconfig
resource "null_resource" "get_kubeconfig" {
  depends_on = [aws_ssm_association.k3s_install]

  provisioner "local-exec" {
    command = <<-EOT
      # Wait for SSM association to complete and K3s to start
      sleep 120
      
      # Create output directory
      mkdir -p ${path.module}/output
      
      # Use SSM to get kubeconfig
      aws ssm start-session \
        --target ${data.aws_instance.existing_instance.id} \
        --document-name AWS-RunShellScript \
        --parameters 'commands=["cat /tmp/kubeconfig"]' \
        --output text > ${path.module}/output/kubeconfig.tmp || echo "Failed to get kubeconfig via SSM"
      
      # Clean up the output file (remove SSM session output headers/footers)
      grep -v "Starting session with SessionId" ${path.module}/output/kubeconfig.tmp | grep -v "Waiting for connections" > ${path.module}/output/kubeconfig || echo "Failed to clean kubeconfig"
      
      # Make the helper script executable
      chmod +x ${path.module}/ssm_commands.sh
    EOT
  }
}

resource "null_resource" "kubeconfig_update" {
  depends_on = [null_resource.get_kubeconfig]

  provisioner "local-exec" {
    command = "sed -i.bak 's/127.0.0.1/${data.aws_instance.existing_instance.private_ip}/g' ${path.module}/output/kubeconfig"
  }
}

output "k3s_server_ip" {
  value = data.aws_instance.existing_instance.private_ip
  description = "Private IP address of the K3s server"
}

output "kubeconfig_path" {
  value = "${path.module}/output/kubeconfig"
}

output "instance_id" {
  value = data.aws_instance.existing_instance.id
  description = "EC2 instance ID for SSM connections"
}

output "ssm_connection_command" {
  value = "aws ssm start-session --target ${data.aws_instance.existing_instance.id}"
  description = "Command to start an SSM session with the instance"
}

output "ssm_helper_script" {
  value = "Run: ${path.module}/ssm_commands.sh ${data.aws_instance.existing_instance.id}"
  description = "Helper script to interact with the K3s cluster via SSM"
}